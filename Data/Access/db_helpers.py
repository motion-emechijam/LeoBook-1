# db_helpers.py: High-level database access layers for LeoBook.
# Refactored for Clean Architecture (v2.7)
# This script manages CSV initialization and structured data saving.

"""
Database Helpers Module
High-level database operations for managing match data and predictions.
Responsible for saving predictions, schedules, standings, teams, and region-leagues.
"""

import os
import csv
from datetime import datetime as dt
from typing import Dict, Any, List, Optional
import uuid

from .csv_operations import _read_csv, _append_to_csv, _write_csv, upsert_entry

# --- Data Store Paths ---
_current_dir = os.path.dirname(os.path.abspath(__file__))
_project_root = os.path.abspath(os.path.join(_current_dir, "..", ".."))
DB_DIR = os.path.join(_project_root, "Data", "Store")
PREDICTIONS_CSV = os.path.join(DB_DIR, "predictions.csv")
SCHEDULES_CSV = os.path.join(DB_DIR, "schedules.csv")
STANDINGS_CSV = os.path.join(DB_DIR, "standings.csv")
TEAMS_CSV = os.path.join(DB_DIR, "teams.csv")
REGION_LEAGUE_CSV = os.path.join(DB_DIR, "region_league.csv")
ACCURACY_REPORTS_CSV = os.path.join(DB_DIR, "accuracy_reports.csv")
FB_MATCHES_CSV = os.path.join(DB_DIR, "fb_matches.csv")
MATCH_REGISTRY_CSV = FB_MATCHES_CSV  # Alias for URL resolution
AUDIT_LOG_CSV = os.path.join(DB_DIR, "audit_log.csv")
PROFILES_CSV = os.path.join(DB_DIR, "profiles.csv")
CUSTOM_RULES_CSV = os.path.join(DB_DIR, "custom_rules.csv")
RULE_EXECUTIONS_CSV = os.path.join(DB_DIR, "rule_executions.csv")


def init_csvs():
    """Initializes all CSV database files."""
    print("     Initializing databases...")
    os.makedirs(DB_DIR, exist_ok=True)

    files_to_init = files_and_headers.copy()
    for filepath, headers in files_to_init.items():
        if not os.path.exists(filepath) or os.path.getsize(filepath) == 0:
            print(f"    [Init] Creating {os.path.basename(filepath)}...")
            with open(filepath, 'w', newline='', encoding='utf-8') as f:
                writer = csv.writer(f)
                writer.writerow(headers)

def log_audit_event(event_type: str, description: str, balance_before: Optional[float] = None, balance_after: Optional[float] = None, stake: Optional[float] = None, status: str = 'success'):
    """Logs a financial or system event to audit_log.csv."""
    row = {
        'id': str(uuid.uuid4()),
        'timestamp': dt.now().strftime("%Y-%m-%d %H:%M:%S"),
        'event_type': event_type,
        'description': description,
        'balance_before': balance_before if balance_before is not None else '',
        'balance_after': balance_after if balance_after is not None else '',
        'stake': stake if stake is not None else '',
        'status': status
    }
    _append_to_csv(AUDIT_LOG_CSV, row, ['id', 'timestamp', 'event_type', 'description', 'balance_before', 'balance_after', 'stake', 'status'])

def save_prediction(match_data: Dict[str, Any], prediction_result: Dict[str, Any]):
    """UPSERTs a prediction into the predictions.csv file."""
    fixture_id = match_data.get('id', 'unknown')
    date = match_data.get('date', dt.now().strftime("%d.%m.%Y"))

    new_row_data = {
        'fixture_id': fixture_id,
        'date': date,
        'match_time': match_data.get('time', '00:00'),
        'region_league': match_data.get('region_league', 'Unknown'),
        'home_team': match_data.get('home_team', 'Unknown'),
        'away_team': match_data.get('away_team', 'Unknown'),
        'home_team_id': match_data.get('home_team_id', 'unknown'),
        'away_team_id': match_data.get('away_team_id', 'unknown'),
        'prediction': prediction_result.get('type', 'SKIP'),
        'confidence': prediction_result.get('confidence', 'Low'),
        'reason': " | ".join(prediction_result.get('reason', [])),
        'xg_home': str(prediction_result.get('xg_home', 0.0)),
        'xg_away': str(prediction_result.get('xg_away', 0.0)),
        'btts': prediction_result.get('btts', '50/50'),
        'over_2.5': prediction_result.get('over_2.5', '50/50'),
        'best_score': prediction_result.get('best_score', '1-1'),
        'top_scores': "|".join([f"{s['score']}({s['prob']})" for s in prediction_result.get('top_scores', [])]),
        'home_tags': "|".join(prediction_result.get('home_tags', [])),
        'away_tags': "|".join(prediction_result.get('away_tags', [])),
        'h2h_tags': "|".join(prediction_result.get('h2h_tags', [])),
        'standings_tags': "|".join(prediction_result.get('standings_tags', [])),
        'h2h_count': str(prediction_result.get('h2h_n', 0)),
        'home_form_n': str(prediction_result.get('home_form_n', 0)),
        'away_form_n': str(prediction_result.get('away_form_n', 0)),
        'generated_at': dt.now().isoformat(),
        'status': 'pending',
        'match_link': f"{match_data.get('match_link', '')}",
        'odds': str(prediction_result.get('odds', '')),
        'market_reliability_score': str(prediction_result.get('market_reliability', 0.0)),
        'home_crest_url': get_team_crest(match_data.get('home_team_id'), match_data.get('home_team')),
        'away_crest_url': get_team_crest(match_data.get('away_team_id'), match_data.get('away_team')),
        'h2h_fixture_ids': json.dumps(prediction_result.get('h2h_fixture_ids', [])),
        'form_fixture_ids': json.dumps(prediction_result.get('form_fixture_ids', [])),
        'standings_snapshot': json.dumps(prediction_result.get('standings_snapshot', [])),
        'league_id': match_data.get('league_id', ''),
        'last_updated': dt.now().isoformat()
    }

    upsert_entry(PREDICTIONS_CSV, new_row_data, files_and_headers[PREDICTIONS_CSV], 'fixture_id')

def update_prediction_status(match_id: str, date: str, new_status: str, **kwargs):
    """
    Updates the status and optional fields (like odds or booking_code) in predictions.csv.
    """
    if not os.path.exists(PREDICTIONS_CSV):
        return

    rows = []
    updated = False
    try:
        with open(PREDICTIONS_CSV, 'r', newline='', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            fieldnames = reader.fieldnames
            for row in reader:
                if row.get('fixture_id') == match_id and row.get('date') == date:
                    row['status'] = new_status
                    row['last_updated'] = dt.now().isoformat()
                    for key, value in kwargs.items():
                        if key in row:
                            row[key] = value
                    updated = True
                rows.append(row)

        if updated and fieldnames is not None:
            _write_csv(PREDICTIONS_CSV, rows, list(fieldnames))
    except Exception as e:
        print(f"    [Warning] Failed to update status for {match_id}: {e}")

def backfill_prediction_entry(fixture_id: str, updates: Dict[str, str]):
    """
    Partially updates an existing prediction row without overwriting analysis data.
    Only updates fields that are currently empty, 'Unknown', or 'N/A'.
    """
    if not fixture_id or not updates:
        return False

    if not os.path.exists(PREDICTIONS_CSV):
        return False

    rows = []
    updated = False
    try:
        with open(PREDICTIONS_CSV, 'r', newline='', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            fieldnames = reader.fieldnames
            for row in reader:
                if row.get('fixture_id') == fixture_id:
                    for key, value in updates.items():
                        if key in row and value:
                            current = row[key].strip() if row[key] else ''
                            if not current or current in ('Unknown', 'N/A', 'unknown'):
                                row[key] = value
                                row['last_updated'] = dt.now().isoformat()
                                updated = True
                    rows.append(row)
                else:
                    rows.append(row)

        if updated and fieldnames is not None:
            _write_csv(PREDICTIONS_CSV, rows, list(fieldnames))
    except Exception as e:
        print(f"    [Warning] Failed to backfill prediction {fixture_id}: {e}")

    return updated

def save_schedule_entry(match_info: Dict[str, Any]):
    # Ensure league_id is present if missing from match_info
    if 'league_id' not in match_info:
        match_info['league_id'] = ''

    # Ensure last_updated is present
    match_info['last_updated'] = dt.now().isoformat()

    upsert_entry(SCHEDULES_CSV, match_info, files_and_headers[SCHEDULES_CSV], 'fixture_id')

def save_standings(standings_data: List[Dict[str, Any]], region_league: str, league_id: str = ""):
    """UPSERTs standings data for a specific league in standings.csv."""
    if not standings_data: return

    last_updated = dt.now().isoformat()
    updated_count = 0

    for row in standings_data:
        row['region_league'] = region_league or row.get('region_league', 'Unknown')
        row['last_updated'] = last_updated
        
        # Priority 1: Provided league_id
        # Priority 2: row['league_id'] (already parsed)
        # Priority 3: Extract from region_league
        l_id = league_id or row.get('league_id', '')
        if not l_id and region_league and " - " in region_league:
             l_id = region_league.split(" - ")[1].replace(' ', '_').upper()
        
        row['league_id'] = l_id
        t_id = row.get('team_id', '')

        # Unique key is now team_id + league_id
        if t_id and l_id:
            row['standings_key'] = f"{l_id}_{t_id}".upper()
            upsert_entry(STANDINGS_CSV, row, files_and_headers[STANDINGS_CSV], 'standings_key')
            updated_count += 1

    if updated_count > 0:
        print(f"      [DB] UPSERTed {updated_count} standings entries for {region_league or league_id}")

def _standardize_url(url: str, base_type: str = "flashscore") -> str:
    """Ensures URLs are absolute and follow standard patterns."""
    if not url or url == 'N/A' or url.startswith("data:"):
        return url
    
    # Handle relative URLs
    if url.startswith("/"):
        url = f"https://www.flashscore.com{url}"
    
    # Standardize team URLs: https://www.flashscore.com/team/{slug}/{id}/
    if "/team/" in url and "https://www.flashscore.com/team/" not in url:
        clean_path = url.split("team/")[-1].strip("/")
        url = f"https://www.flashscore.com/team/{clean_path}/"
    elif "/team/" in url:
        # Ensure trailing slash for team URLs
        if not url.endswith("/"): url += "/"

    # Standardize league/region URLs: ensure absolute
    if "flashscore.com" not in url and not url.startswith("http"):
        url = f"https://www.flashscore.com{url if url.startswith('/') else '/' + url}"

    return url

def save_region_league_entry(info: Dict[str, Any]):
    """Saves or updates a single region-league entry in region_league.csv."""
    rl_id = info.get('rl_id')
    
    # Validation: rl_id should preferentially be the fragment hash if available
    region = info.get('region', 'Unknown')
    league = info.get('league', 'Unknown')
    if not rl_id:
        rl_id = f"{region}_{league}".replace(' ', '_').replace('-', '_').upper()

    entry = {
        'rl_id': rl_id,
        'region': region,
        'region_flag': _standardize_url(info.get('region_flag', '')),
        'region_url': _standardize_url(info.get('region_url', '')),
        'league': league,
        'league_crest': _standardize_url(info.get('league_crest', '')),
        'league_url': _standardize_url(info.get('league_url', '')),
        'date_updated': dt.now().isoformat(),
        'last_updated': dt.now().isoformat()
    }

    upsert_entry(REGION_LEAGUE_CSV, entry, files_and_headers[REGION_LEAGUE_CSV], 'rl_id')


def save_team_entry(team_info: Dict[str, Any]):
    """Saves or updates a single team entry in teams.csv with multi-league support."""
    team_id = team_info.get('team_id')
    if not team_id or team_id == 'unknown': return

    # Check for existing entry to merge rl_ids
    existing_rows = _read_csv(TEAMS_CSV)
    new_rl_id = team_info.get('rl_ids', team_info.get('region_league', ''))
    
    merged_rl_ids = new_rl_id
    for row in existing_rows:
        if row.get('team_id') == team_id:
            existing_rl_ids = row.get('rl_ids', '').split(';')
            if new_rl_id and new_rl_id not in existing_rl_ids:
                existing_rl_ids.append(new_rl_id)
            merged_rl_ids = ';'.join(filter(None, existing_rl_ids))
            break

    entry = {
        'team_id': team_id,
        'team_name': team_info.get('team_name', 'Unknown'),
        'rl_ids': merged_rl_ids,
        'team_crest': _standardize_url(team_info.get('team_crest', '')),
        'team_url': _standardize_url(team_info.get('team_url', '')),
        'last_updated': dt.now().isoformat()
    }

    upsert_entry(TEAMS_CSV, entry, files_and_headers[TEAMS_CSV], 'team_id')

def get_team_crest(team_id: str, team_name: str = "") -> str:
    """Retrieves the crest URL for a team from teams.csv."""
    if not os.path.exists(TEAMS_CSV):
        return ""
    
    rows = _read_csv(TEAMS_CSV)
    for row in rows:
        if str(row.get('team_id')) == str(team_id) or (team_name and row.get('team_name') == team_name):
            return row.get('team_crest', '')
    return ""

# --- Football.com Registry Helpers ---

def get_site_match_id(date: str, home: str, away: str) -> str:
    """Generate a unique ID for a site match to prevent duplicates."""
    import hashlib
    unique_str = f"{date}_{home}_{away}".lower().strip()
    return hashlib.md5(unique_str.encode()).hexdigest()

def save_site_matches(matches: List[Dict[str, Any]]):
    """UPSERTs a list of matches extracted from Football.com into the registry."""
    if not matches: return
    
    headers = files_and_headers[FB_MATCHES_CSV]
    last_extracted = dt.now().isoformat()
    
    for match in matches:
        site_id = get_site_match_id(match.get('date', ''), match.get('home', ''), match.get('away', ''))
        row = {
            'site_match_id': site_id,
            'date': match.get('date'),
            'time': match.get('time', 'N/A'),
            'home_team': match.get('home'),
            'away_team': match.get('away'),
            'league': match.get('league'),
            'url': match.get('url'),
            'last_extracted': last_extracted,
            'fixture_id': match.get('fixture_id', ''),
            'matched': match.get('matched', 'No_fs_match_found'),
            'booking_status': match.get('booking_status', 'pending'),
            'booking_details': match.get('booking_details', ''),
            'booking_code': match.get('booking_code', ''),
            'booking_url': match.get('booking_url', ''),
            'status': match.get('status', ''),
            'last_updated': dt.now().isoformat()
        }
        upsert_entry(FB_MATCHES_CSV, row, headers, 'site_match_id')

def load_site_matches(target_date: str) -> List[Dict[str, Any]]:
    """Loads all extracted site matches for a specific date."""
    if not os.path.exists(FB_MATCHES_CSV):
        return []
    
    all_matches = _read_csv(FB_MATCHES_CSV)
    return [m for m in all_matches if m.get('date') == target_date]

def load_harvested_site_matches(target_date: str) -> List[Dict[str, Any]]:
    """Loads all harvested site matches for a specific date (v2.7)."""
    if not os.path.exists(FB_MATCHES_CSV):
        return []
    
    all_matches = _read_csv(FB_MATCHES_CSV)
    return [m for m in all_matches if m.get('date') == target_date and m.get('booking_status') == 'harvested']

def update_site_match_status(site_match_id: str, status: str, fixture_id: Optional[str] = None, details: Optional[str] = None, booking_code: Optional[str] = None, booking_url: Optional[str] = None, matched: Optional[str] = None, **kwargs):
    """Updates the booking status, fixture_id, or booking details for a site match."""
    if not os.path.exists(FB_MATCHES_CSV):
        return

    rows = []
    updated = False
    try:
        with open(FB_MATCHES_CSV, 'r', newline='', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            fieldnames = reader.fieldnames
            for row in reader:
                if row.get('site_match_id') == site_match_id:
                    row['booking_status'] = status
                    if fixture_id: row['fixture_id'] = fixture_id
                    if details: row['booking_details'] = details
                    if booking_code: row['booking_code'] = booking_code
                    if booking_url: row['booking_url'] = booking_url
                    if status: row['status'] = status
                    if matched: row['matched'] = matched
                    if 'odds' in kwargs: row['odds'] = kwargs['odds']
                    updated = True
                rows.append(row)

        if updated and fieldnames is not None:
            _write_csv(FB_MATCHES_CSV, rows, list(fieldnames))
    except Exception as e:
        print(f"    [DB Error] Failed to update site match status: {e}")

def get_last_processed_info() -> Dict:
    """Loads last processed match info once at the start."""
    last_processed_info = {}
    if os.path.exists(PREDICTIONS_CSV):
        try:
            all_predictions = _read_csv(PREDICTIONS_CSV)
            if all_predictions:
                last_prediction = all_predictions[-1]
                date_str = last_prediction.get('date')
                if date_str:
                    last_processed_info = {
                        'date': date_str,
                        'id': last_prediction.get('fixture_id'),
                        'date_obj': dt.strptime(date_str, "%d.%m.%Y").date()
                    }
                    print(f"    [Resume] Last processed: ID {last_processed_info['id']} on {last_processed_info['date']}")
        except Exception as e:
            print(f"    [Warning] Could not read CSV for resume check: {e}")
    return last_processed_info

def get_all_schedules() -> List[Dict[str, Any]]:
    """Loads all match schedules from schedules.csv."""
    return _read_csv(SCHEDULES_CSV)

def get_standings(region_league: str) -> List[Dict[str, Any]]:
    """Loads standings for a specific league from standings.csv."""
    all_standings = _read_csv(STANDINGS_CSV)
    return [s for s in all_standings if s.get('region_league') == region_league]

# To be accessible from other modules, we need to define the headers dict here
files_and_headers = {
    PREDICTIONS_CSV: [
        'fixture_id', 'date', 'match_time', 'region_league', 'home_team', 'away_team', 
        'home_team_id', 'away_team_id', 'prediction', 'confidence', 'reason', 'xg_home', 
        'xg_away', 'btts', 'over_2.5', 'best_score', 'top_scores', 'home_form_n', 
        'away_form_n', 'home_tags', 'away_tags', 'h2h_tags', 'standings_tags', 
        'h2h_count', 'form_count', 'actual_score', 'outcome_correct', 
        'generated_at', 'status', 'match_link', 'odds', 'market_reliability_score',
        'home_crest_url', 'away_crest_url', 'is_recommended', 'recommendation_score',
        'h2h_fixture_ids', 'form_fixture_ids', 'standings_snapshot', 'league_id', 
        'league_stage', 'last_updated'
    ],
    SCHEDULES_CSV: [
        'fixture_id', 'date', 'match_time', 'region_league', 'league_id', 'home_team', 'away_team',
        'home_team_id', 'away_team_id', 'home_score', 'away_score', 'match_status', 
        'match_link', 'league_stage', 'last_updated'
    ],
    STANDINGS_CSV: [
        'standings_key', 'league_id', 'team_id', 'team_name', 'position', 'played', 'wins', 'draws',
        'losses', 'goals_for', 'goals_against', 'goal_difference', 'points', 
        'last_updated', 'url', 'region_league'
    ],
    TEAMS_CSV: ['team_id', 'team_name', 'rl_ids', 'team_crest', 'team_url', 'last_updated'],
    REGION_LEAGUE_CSV: ['rl_id', 'region', 'region_flag', 'region_url', 'league', 'league_crest', 'league_url', 'date_updated', 'last_updated'],
    ACCURACY_REPORTS_CSV: ['report_id', 'timestamp', 'volume', 'win_rate', 'return_pct', 'period', 'last_updated'],
    FB_MATCHES_CSV: [
        'site_match_id', 'date', 'time', 'home_team', 'away_team', 'league', 'url', 
        'last_extracted', 'fixture_id', 'matched', 'odds', 'booking_status', 'booking_details',
        'booking_code', 'booking_url', 'status', 'last_updated'
    ],
    AUDIT_LOG_CSV: [
        'timestamp', 'event_type', 'description', 'balance_before', 'balance_after', 'stake', 'status'
    ],
    # User & Rule Engine Tables
    os.path.join(DB_DIR, "profiles.csv"): [
        'id', 'email', 'username', 'full_name', 'avatar_url', 'tier', 'credits', 'created_at', 'updated_at', 'last_updated'
    ],
    os.path.join(DB_DIR, "custom_rules.csv"): [
        'id', 'user_id', 'name', 'description', 'is_active', 'logic', 'priority', 'created_at', 'updated_at', 'last_updated'
    ],
    os.path.join(DB_DIR, "rule_executions.csv"): [
        'id', 'rule_id', 'fixture_id', 'user_id', 'result', 'executed_at', 'last_updated'
    ]
}
